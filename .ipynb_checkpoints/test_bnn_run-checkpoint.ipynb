{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaming/mambaforge/envs/brevitas/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model_weights = torch.load(\"mnist_bnn_mlp.pt\")\n",
    "print(type(model_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['fc1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'fc2.weight', 'bn2.weight', 'bn2.bias', 'bn2.running_mean', 'bn2.running_var', 'bn2.num_batches_tracked', 'fc3.weight', 'bn3.weight', 'bn3.bias', 'bn3.running_mean', 'bn3.running_var', 'bn3.num_batches_tracked', 'fc4.weight', 'bn4.weight', 'bn4.bias', 'bn4.running_mean', 'bn4.running_var', 'bn4.num_batches_tracked'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fc1.weight', array([[ 0.01796702, -0.01619875, -0.00735881, ...,  0.00634418,\n",
      "        -0.00452901,  0.00206897],\n",
      "       [-0.02773761, -0.02280657, -0.0312873 , ..., -0.00328715,\n",
      "         0.00089431, -0.01704787],\n",
      "       [ 0.02805158, -0.00959798, -0.03416752, ..., -0.01420229,\n",
      "         0.00550288, -0.03365139],\n",
      "       ...,\n",
      "       [ 0.00570961,  0.0349777 , -0.03417083, ...,  0.03363043,\n",
      "         0.02686312, -0.02106128],\n",
      "       [ 0.01226201,  0.02846391, -0.00309211, ...,  0.03344198,\n",
      "         0.033974  ,  0.01770352],\n",
      "       [ 0.03216547, -0.02721824,  0.02646583, ...,  0.00504588,\n",
      "         0.01041278,  0.02281732]], dtype=float32))\n",
      "('bn1.weight', array([1.4778259, 2.5675004, 1.9303343, ..., 2.3447855, 1.886667 ,\n",
      "       2.1504722], dtype=float32))\n",
      "('bn1.bias', array([ 1.8485988 , -0.62126714, -0.0982881 , ..., -0.568802  ,\n",
      "       -0.9123156 , -0.19557329], dtype=float32))\n",
      "('bn1.running_mean', array([ 51.274117,  62.234627, -26.783463, ..., -45.81591 ,  34.40998 ,\n",
      "        23.768795], dtype=float32))\n",
      "('bn1.running_var', array([1567.486 , 1709.7561, 1650.7996, ..., 1993.0249, 1306.2604,\n",
      "       1185.3082], dtype=float32))\n",
      "('bn1.num_batches_tracked', array(2350))\n",
      "('fc2.weight', array([[ 0.21911146,  0.42440584, -0.39533997, ...,  0.4593983 ,\n",
      "         0.31432796, -0.08164512],\n",
      "       [ 0.766455  ,  0.79672456, -0.2771674 , ..., -0.41593152,\n",
      "        -0.22986042,  0.3277957 ],\n",
      "       [-0.15132181,  1.2788631 , -0.24174945, ...,  0.05493925,\n",
      "        -0.4743213 ,  0.22268541],\n",
      "       ...,\n",
      "       [ 0.22673091,  0.08023035, -0.37745845, ...,  0.66170657,\n",
      "         0.07118522, -0.14891545],\n",
      "       [ 0.06651138,  0.01378   ,  0.4667327 , ...,  0.10373806,\n",
      "         0.17832352, -0.14150889],\n",
      "       [ 0.30259982,  0.11213509, -0.27263412, ...,  0.32954076,\n",
      "         0.11163709,  0.29261264]], dtype=float32))\n",
      "('bn2.weight', array([ 0.6652267 ,  0.52208716,  1.9437563 , ...,  1.8377472 ,\n",
      "        1.4002118 , -0.08128323], dtype=float32))\n",
      "('bn2.bias', array([ 0.8042885 , -0.6610353 ,  0.24949893, ...,  0.40135655,\n",
      "       -0.60206586, -0.07567201], dtype=float32))\n",
      "('bn2.running_mean', array([  11.6616125,  -14.973494 , -130.08017  , ...,  -22.197453 ,\n",
      "         38.495274 ,   31.574354 ], dtype=float32))\n",
      "('bn2.running_var', array([ 5908.523 ,  4212.9546,  4104.89  , ...,  4483.6626,  7121.904 ,\n",
      "       15550.658 ], dtype=float32))\n",
      "('bn2.num_batches_tracked', array(2350))\n",
      "('fc3.weight', array([[ 0.19570087,  0.02471698, -0.01579671, ..., -0.31492388,\n",
      "        -0.19387335,  0.08132253],\n",
      "       [-0.4478438 , -0.29465201,  0.16795468, ..., -0.28926277,\n",
      "        -0.10094639,  0.23061924],\n",
      "       [ 0.33836895,  0.5460437 , -0.24329233, ..., -0.07646704,\n",
      "        -0.5994992 , -0.13960563],\n",
      "       ...,\n",
      "       [-0.27379137,  0.09450501, -0.6495647 , ..., -0.04717947,\n",
      "        -0.27817398, -0.49558166],\n",
      "       [ 0.10650653, -0.17804156,  0.15380655, ..., -0.24373476,\n",
      "         0.07028644, -0.18188281],\n",
      "       [ 0.01509372, -0.00111945,  0.47362626, ...,  0.5882937 ,\n",
      "         0.28257096,  0.00945937]], dtype=float32))\n",
      "('bn3.weight', array([1.2099371, 2.089092 , 1.3179588, ..., 1.0100912, 1.1237695,\n",
      "       1.2314361], dtype=float32))\n",
      "('bn3.bias', array([ 0.14980136, -0.45330843, -0.46132803, ...,  0.73532116,\n",
      "        0.7410826 , -1.041859  ], dtype=float32))\n",
      "('bn3.running_mean', array([ 11.941282, -18.300154,   9.187733, ...,  10.945843,  38.037693,\n",
      "       -21.708103], dtype=float32))\n",
      "('bn3.running_var', array([10148.986 , 13047.315 , 10753.287 , ...,  8159.8433,  7992.9365,\n",
      "        7704.2173], dtype=float32))\n",
      "('bn3.num_batches_tracked', array(2350))\n",
      "('fc4.weight', array([[-0.31639373,  0.7859708 , -0.40319467, ..., -0.02070877,\n",
      "        -0.14722323, -0.00403134],\n",
      "       [-0.09013564,  0.16277884, -0.2282944 , ...,  0.0330485 ,\n",
      "        -0.35410833, -0.746611  ],\n",
      "       [-0.09893712,  0.8166846 , -0.05644261, ..., -0.6194215 ,\n",
      "        -0.4402775 ,  0.5258207 ],\n",
      "       ...,\n",
      "       [ 0.28715846, -0.54922634, -0.6019248 , ..., -0.06538437,\n",
      "         0.8346324 ,  0.5335309 ],\n",
      "       [-0.47211504, -0.34576142, -0.40648955, ..., -0.25684196,\n",
      "         0.54953104,  0.5230089 ],\n",
      "       [-0.11293255, -0.05766992,  0.05514147, ..., -0.04519447,\n",
      "        -0.83278406,  0.01690113]], dtype=float32))\n",
      "('bn4.weight', array([3.6245255, 3.8739142, 3.494716 , 3.2447162, 3.3829975, 3.2482839,\n",
      "       3.6055806, 3.480834 , 3.1007802, 3.1333976], dtype=float32))\n",
      "('bn4.bias', array([-0.44250295,  0.02800531, -0.03408566,  0.16020775, -0.11172114,\n",
      "       -0.08278409, -0.27612793,  0.09087741,  0.1613969 ,  0.31087747],\n",
      "      dtype=float32))\n",
      "('bn4.running_mean', array([-23.161507 , -57.160835 , -79.89243  ,  -2.3491354, -45.801132 ,\n",
      "       -35.969265 , -49.889767 , -49.800686 ,   4.191356 , -91.87346  ],\n",
      "      dtype=float32))\n",
      "('bn4.running_var', array([16307.662, 17055.396, 14775.605, 18764.71 , 19298.504, 16012.178,\n",
      "       13358.671, 13625.183, 19058.588, 15753.384], dtype=float32))\n",
      "('bn4.num_batches_tracked', array(2350))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for layer in model_weights.items():\n",
    "    model_weights[layer[0]] = layer[1].cpu().numpy()\n",
    "\n",
    "for layer in model_weights.items():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_weights.items())[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinaryLinear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = np.zeros([out_features, in_features])\n",
    "        self.binary_weight = np.zeros([out_features, in_features])\n",
    "    \n",
    "    def binarize(self):\n",
    "        self.binary_weight = np.ones(self.weight.shape)\n",
    "        self.binary_weight[self.weight<0] = -1\n",
    "        self.binary_weight = (self.binary_weight + np.ones(self.weight.shape))/2\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        output = np.zeros([1, self.out_features])\n",
    "        for i in range(self.out_features):\n",
    "            row = self.binary_weight[i,:]\n",
    "            xnor = ~np.logical_xor(row, x)\n",
    "            output[0, i] = 2*np.sum(xnor) - self.in_features\n",
    "        return output\n",
    "\n",
    "class BinaryHardTanH:\n",
    "    def __init__(self, in_features):\n",
    "        self.in_features = in_features\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x[x>1] = 1\n",
    "        x[x<-1] = -1\n",
    "        x[x<0] = -1\n",
    "        x[x>=0] = 1\n",
    "        return (x+np.ones(x.shape))/2\n",
    "        \n",
    "        \n",
    "\n",
    "class BinaryNet:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.features = [\n",
    "            BinaryLinear(784,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            BinaryHardTanH(1024),\n",
    "            \n",
    "            BinaryLinear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            BinaryHardTanH(1024),\n",
    "            \n",
    "            BinaryLinear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            BinaryHardTanH(1024),\n",
    "            \n",
    "            BinaryLinear(1024,10),\n",
    "            nn.BatchNorm1d(10)\n",
    "        ]\n",
    "    \n",
    "    def load_weight(self, weight):\n",
    "        cnt = 0\n",
    "        \n",
    "        weight = list(weight.items())\n",
    "        for layer in self.features:\n",
    "            (key, value) = weight[cnt]\n",
    "\n",
    "            if isinstance(layer, BinaryHardTanH):\n",
    "                continue\n",
    "            \n",
    "            if layer.weight.shape != value.shape:\n",
    "                print(\"Non competible shape, expected: {}, loading: {}\".format(layer.weight.shape, value.shape))\n",
    "                raise\n",
    "                \n",
    "            layer.weight = nn.Parameter(torch.tensor(value))\n",
    "            cnt+=1\n",
    "            \n",
    "            if 'bn' in key:\n",
    "                (key, value) = weight[cnt]\n",
    "                layer.bias = nn.Parameter(torch.tensor(value))\n",
    "                \n",
    "                (key, value) = weight[cnt+1]\n",
    "                layer.running_mean = torch.tensor(value)\n",
    "                \n",
    "                (key, value) = weight[cnt+2]\n",
    "                layer.running_var = torch.tensor(value)\n",
    "                \n",
    "                cnt+=4\n",
    "    \n",
    "    def binarize(self):\n",
    "        for layer in self.features:\n",
    "            if isinstance(layer, BinaryLinear):\n",
    "                layer.binarize()\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # turn to zero one\n",
    "        x[x<0] = -1; x[x>=0] = 1\n",
    "        x = (x + np.ones(x.shape))/2\n",
    "        \n",
    "        for layer in self.features:\n",
    "            if isinstance(layer, nn.BatchNorm1d):\n",
    "                x = torch.tensor(x, dtype=torch.float32)\n",
    "                layer.training = False\n",
    "                x = layer(x)\n",
    "                x = x.detach().numpy()\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = BinaryNet()\n",
    "bnn.load_weight(model_weights)\n",
    "bnn.binarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.7260,  -3.0064,  -0.0905,  -4.8599, -11.4987,  -4.2112,  -6.5674,\n",
       "          -4.7578, -10.1762,  -5.7181]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.random.uniform(-1,1,[1,784]).astype(np.float32)\n",
    "\n",
    "print(s.shape)\n",
    "bnn(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "test_kwargs = {'batch_size': 1}\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [03:38<00:00, 45.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 97.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "from tqdm import tqdm\n",
    "for data, target in tqdm(test_loader):\n",
    "    data = data.flatten(start_dim=1).numpy()\n",
    "    output = bnn(data)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "print(\"Accuracy = {}\".format(100. * correct / len(test_loader.dataset)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/jiaming/mambaforge/lib/python3.10/site-packages (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.uniform(1,-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5282, -0.4630, -0.3560,  0.3242,  0.2986,  0.9084, -0.6866, -0.0322,\n",
       "         -0.6632,  0.3347, -0.8971,  0.9783,  0.1746,  0.2339,  0.9170, -0.8043,\n",
       "          0.8858,  0.5832,  0.8288,  0.6002,  0.0873,  0.1066, -0.6711,  0.7424,\n",
       "          0.0589, -0.9383,  0.1378, -0.1173,  0.5028, -0.7691,  0.2411, -0.0311,\n",
       "          0.5974, -0.7782, -0.3050, -0.2316, -0.8062, -0.2975, -0.1959, -0.1657,\n",
       "          0.8577, -0.4954, -0.1729, -0.3457,  0.9202, -0.5183,  0.5251,  0.7135,\n",
       "         -0.5186,  0.0313,  0.3776, -0.5130,  0.9162,  0.3030,  0.0122,  0.0909,\n",
       "          0.1205, -0.0144,  0.9688, -0.7357, -0.8532, -0.5544,  0.2906, -0.7739,\n",
       "         -0.5668, -0.2397, -0.9000, -0.0629,  0.0100, -0.7840, -0.6264,  0.2224,\n",
       "         -0.5520, -0.6755, -0.1389, -0.7568, -0.7120, -0.9943,  0.7059, -0.2137,\n",
       "          0.3697, -0.9765, -0.2583,  0.2644,  0.7865,  0.3486,  0.2491,  0.5264,\n",
       "          0.1822, -0.9926,  0.7163,  0.2418,  0.9793, -0.0218,  0.1499, -0.9348,\n",
       "          0.3937,  0.1368,  0.1933, -0.9908, -0.0406,  0.7417,  0.5751, -0.4768,\n",
       "         -0.4876, -0.7265, -0.9153,  0.8593,  0.6747,  0.8568, -0.7516,  0.2131,\n",
       "          0.6988,  0.6261, -0.8671,  0.0937,  0.3611, -0.2142,  0.0014,  0.9893,\n",
       "         -0.8628,  0.6778, -0.3452, -0.6384, -0.3030,  0.6976, -0.8126,  0.0374,\n",
       "          0.2521,  0.0112,  0.2579,  0.8328, -0.4859, -0.3040,  0.8507, -0.3704,\n",
       "         -0.9284, -0.9175, -0.8694, -0.9624,  0.0552,  0.4014, -0.4649,  0.2608,\n",
       "          0.3528, -0.1300, -0.8374,  0.4060,  0.0693, -0.8745,  0.1197,  0.2500,\n",
       "          0.7264,  0.4752,  0.9059, -0.9024,  0.0828, -0.9780, -0.4988, -0.2412,\n",
       "         -0.1854,  0.8857,  0.5163, -0.7999, -0.8995, -0.9230,  0.8262,  0.5689,\n",
       "          0.6998,  0.3711,  0.4104, -0.6466,  0.7797,  0.8909,  0.7495, -0.0719,\n",
       "         -0.4961, -0.2170, -0.4355, -0.6376, -0.8935,  0.5152,  0.6966,  0.4830,\n",
       "          0.3164,  0.9881, -0.8686, -0.2506, -0.7341, -0.5729,  0.4558, -0.5700,\n",
       "         -0.6931,  0.1183,  0.1815,  0.1509, -0.5849,  0.8124, -0.7133, -0.2103,\n",
       "          0.5228, -0.6813,  0.3354, -0.5075,  0.8855, -0.1500,  0.8754, -0.3089,\n",
       "         -0.1413, -0.8239,  0.2836, -0.9816, -0.2055, -0.8767, -0.5356,  0.6015,\n",
       "         -0.3108,  0.5843,  0.2365, -0.0623, -0.1248,  0.6359,  0.3354,  0.6726,\n",
       "         -0.4388, -0.5753, -0.0858,  0.3541,  0.4660, -0.3797, -0.6888, -0.4238,\n",
       "         -0.0405, -0.5388, -0.0736,  0.0377, -0.5106, -0.9586,  0.5939, -0.2431,\n",
       "          0.1009, -0.5632,  0.0666, -0.6891, -0.3506,  0.2738, -0.1143,  0.0657,\n",
       "          0.4395,  0.7123,  0.9055, -0.4166, -0.5312,  0.6648, -0.7486,  0.1712,\n",
       "          0.2187,  0.8801, -0.8705,  0.0964,  0.0961, -0.8907, -0.1265, -0.7436,\n",
       "          0.1116,  0.1939,  0.6847,  0.0921,  0.7812,  0.1754, -0.4318,  0.6040,\n",
       "          0.6931,  0.9570, -0.0882,  0.5784,  0.7896, -0.1028, -0.3505, -0.4716,\n",
       "         -0.5780,  0.2129, -0.1811,  0.8488, -0.2099, -0.0864, -0.4871,  0.7883,\n",
       "          0.1236, -0.9060, -0.3404,  0.1055,  0.9010, -0.0490, -0.2607,  0.3336,\n",
       "          0.6011, -0.7729, -0.5171,  0.8871,  0.2172,  0.3114, -0.7044,  0.9094,\n",
       "         -0.8279,  0.4149,  0.6367,  0.9362,  0.6337,  0.7184,  0.5505,  0.0596,\n",
       "          0.6343, -0.8196, -0.4388, -0.7432,  0.6020,  0.3784,  0.6193, -0.5006,\n",
       "         -0.6088, -0.1649, -0.7686,  0.3631,  0.6265, -0.4566,  0.1218, -0.7624,\n",
       "          0.4028, -0.5682, -0.0111, -0.3912, -0.7082,  0.2641,  0.3453, -0.8812,\n",
       "         -0.3688,  0.2311,  0.4413,  0.1900,  0.7385,  0.6778, -0.5586, -0.4427,\n",
       "          0.4962, -0.7477,  0.3809,  0.2079, -0.8310, -0.4000, -0.6087, -0.8784,\n",
       "         -0.4829,  0.3609, -0.4447,  0.1677, -0.8770, -0.7176, -0.6828, -0.3305,\n",
       "         -0.3820, -0.4756, -0.7802, -0.2686,  0.5516, -0.3803,  0.5504,  0.0511,\n",
       "          0.3678,  0.0572,  0.5774, -0.7715,  0.7773, -0.1892, -0.7433, -0.4732,\n",
       "         -0.1755, -0.4983, -0.7821, -0.6432,  0.4285, -0.4611, -0.7868,  0.9599,\n",
       "         -0.9495, -0.6504, -0.9689,  0.9606, -0.7710,  0.1904, -0.9321,  0.8895,\n",
       "          0.2635,  0.6864, -0.2651, -0.9687, -0.3705, -0.4620, -0.5867, -0.6559,\n",
       "         -0.8489, -0.1424,  0.6519, -0.9465, -0.2981,  0.8079, -0.8951, -0.9716,\n",
       "          0.2445,  0.9432,  0.0890, -0.6395, -0.5996,  0.7871, -0.6211,  0.2792,\n",
       "          0.2403,  0.8498, -0.9578, -0.6784,  0.2890, -0.9920, -0.2588, -0.0346,\n",
       "          0.4388,  0.3693, -0.8701, -0.4995,  0.8222,  0.8033, -0.5112,  0.5951,\n",
       "         -0.3212, -0.4217,  0.1015,  0.9019,  0.3705,  0.9305,  0.0032, -0.1202,\n",
       "          0.9997,  0.1244, -0.3607,  0.8128,  0.4575,  0.5482,  0.7181,  0.6064,\n",
       "          0.6218, -0.0750, -0.6970, -0.8830, -0.2975, -0.0022, -0.7226,  0.1982,\n",
       "          0.6065,  0.2875,  0.0272,  0.7911, -0.7519, -0.5675, -0.0687,  0.7533,\n",
       "         -0.7511, -0.6660, -0.5990,  0.1544, -0.9015,  0.8856,  0.6317,  0.9049,\n",
       "          0.3101,  0.3258, -0.4795, -0.3743,  0.5438,  0.6045,  0.8281, -0.9382,\n",
       "          0.2802, -0.4333,  0.9453, -0.1306,  0.2008, -0.0633,  0.0769, -0.5999,\n",
       "         -0.3013,  0.1722, -0.8893, -0.8998, -0.3641,  0.8865, -0.9257, -0.7659,\n",
       "          0.2055,  0.3565, -0.9650, -0.2393, -0.2720,  0.8921, -0.0179, -0.2939,\n",
       "          0.2586,  0.2714,  0.3164, -0.3653,  0.3950, -0.0414, -0.4035,  0.4550,\n",
       "          0.4953,  0.2938, -0.2529,  0.6814,  0.1073,  0.0229, -0.8298, -0.0250,\n",
       "          0.8861, -0.7679, -0.3089,  0.3016,  0.3137, -0.1990, -0.4324, -0.8121,\n",
       "         -0.2226, -0.3443,  0.1962,  0.4493, -0.5159, -0.0960, -0.2175, -0.7747,\n",
       "          0.0408, -0.9016,  0.5527,  0.6492, -0.4661, -0.7110, -0.4736, -0.7626,\n",
       "         -0.6883,  0.2066, -0.3419,  0.1544,  0.9363, -0.5184, -0.4048, -0.2630,\n",
       "          0.2792,  0.7271, -0.7435,  0.8874, -0.1839,  0.4347,  0.2256, -0.3202,\n",
       "         -0.4249, -0.4185,  0.8149,  0.3321, -0.4604, -0.1221, -0.2856, -0.8723,\n",
       "          0.0962, -0.5884, -0.2963,  0.0923,  0.4778, -0.6334, -0.3128, -0.6072,\n",
       "         -0.5077,  0.4380,  0.7342, -0.3118,  0.9246, -0.1376, -0.4629,  0.8791,\n",
       "          0.1201, -0.3231,  0.3419, -0.6325, -0.4408, -0.9678, -0.5258, -0.4852,\n",
       "          0.4627, -0.0931, -0.4206, -0.6621,  0.7391,  0.7168, -0.9091,  0.6148,\n",
       "          0.8601,  0.1509,  0.0176,  0.6408, -0.8543, -0.6362,  0.2971,  0.0858,\n",
       "          0.8401,  0.8918, -0.7093, -0.4132,  0.7304, -0.5273,  0.4490,  0.4575,\n",
       "         -0.2280, -0.0647, -0.7240,  0.8647, -0.2501, -0.2335,  0.3498, -0.6129,\n",
       "          0.3349, -0.4346, -0.2822, -0.0918, -0.0203, -0.9108,  0.7848, -0.0682,\n",
       "          0.3279, -0.2017,  0.5664,  0.2020,  0.9584, -0.0611,  0.5209,  0.5479,\n",
       "          0.3560,  0.0880,  0.8031,  0.2301, -0.3160,  0.0189,  0.2245, -0.4057,\n",
       "         -0.6173,  0.9149, -0.2203,  0.5672, -0.3052, -0.8476,  0.9657,  0.9362,\n",
       "         -0.9250, -0.9340,  0.8779, -0.9542,  0.3986, -0.2833, -0.4991,  0.3498,\n",
       "         -0.6720, -0.5277, -0.2865,  0.1543, -0.6482, -0.4568, -0.2570,  0.7731,\n",
       "         -0.5271, -0.7554,  0.8940,  0.2904,  0.3144,  0.2199, -0.6085,  0.9024,\n",
       "          0.1818,  0.4064,  0.6838,  0.4851,  0.0568,  0.7673, -0.7956, -0.7178,\n",
       "         -0.5705, -0.6466,  0.3554,  0.1937, -0.9668, -0.2217, -0.8420,  0.3719,\n",
       "         -0.4248,  0.2259,  0.1919,  0.5019,  0.7171,  0.4637, -0.8837, -0.3806,\n",
       "         -0.0736, -0.3592, -0.5265, -0.7484,  0.0085, -0.5923,  0.3058, -0.0516,\n",
       "         -0.2587,  0.0319, -0.3505,  0.8076,  0.5343,  0.9653, -0.6668,  0.7409,\n",
       "         -0.2619, -0.2993, -0.9807, -0.0261,  0.7688,  0.1820, -0.2745,  0.5905,\n",
       "         -0.0965,  0.2416, -0.3137,  0.4074,  0.2789, -0.3619,  0.7631, -0.3654,\n",
       "         -0.8941, -0.3341,  0.4254, -0.1211,  0.8945, -0.9469,  0.6001, -0.2900,\n",
       "         -0.9857,  0.6927, -0.7208,  0.7528, -0.7653,  0.0791,  0.2932, -0.8392,\n",
       "         -0.1097,  0.8734, -0.8005,  0.2041, -0.1659,  0.5139,  0.1035, -0.4377,\n",
       "         -0.7923, -0.3588, -0.2387, -0.8141,  0.0473, -0.0691, -0.6504,  0.4247,\n",
       "          0.6914, -0.2527,  0.9042, -0.0115, -0.1336, -0.9151,  0.6041, -0.8270,\n",
       "         -0.9717,  0.7730, -0.8124,  0.4223,  0.2482,  0.2361, -0.2575, -0.0858]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.expand_dims(s, axis=0), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5282, -0.4630, -0.3560,  0.3242,  0.2986,  0.9084, -0.6866, -0.0322,\n",
       "         -0.6632,  0.3347, -0.8971,  0.9783,  0.1746,  0.2339,  0.9170, -0.8043,\n",
       "          0.8858,  0.5832,  0.8288,  0.6002,  0.0873,  0.1066, -0.6711,  0.7424,\n",
       "          0.0589, -0.9383,  0.1378, -0.1173,  0.5028, -0.7691,  0.2411, -0.0311,\n",
       "          0.5974, -0.7782, -0.3050, -0.2316, -0.8062, -0.2975, -0.1959, -0.1657,\n",
       "          0.8577, -0.4954, -0.1729, -0.3457,  0.9202, -0.5183,  0.5251,  0.7135,\n",
       "         -0.5186,  0.0313,  0.3776, -0.5130,  0.9162,  0.3030,  0.0122,  0.0909,\n",
       "          0.1205, -0.0144,  0.9688, -0.7357, -0.8532, -0.5544,  0.2906, -0.7739,\n",
       "         -0.5668, -0.2397, -0.9000, -0.0629,  0.0100, -0.7840, -0.6264,  0.2224,\n",
       "         -0.5520, -0.6755, -0.1389, -0.7568, -0.7120, -0.9943,  0.7059, -0.2137,\n",
       "          0.3697, -0.9765, -0.2583,  0.2644,  0.7865,  0.3486,  0.2491,  0.5264,\n",
       "          0.1822, -0.9926,  0.7162,  0.2418,  0.9793, -0.0218,  0.1499, -0.9348,\n",
       "          0.3937,  0.1368,  0.1933, -0.9908, -0.0406,  0.7417,  0.5751, -0.4768,\n",
       "         -0.4876, -0.7265, -0.9153,  0.8593,  0.6747,  0.8568, -0.7516,  0.2131,\n",
       "          0.6988,  0.6261, -0.8671,  0.0937,  0.3611, -0.2142,  0.0014,  0.9893,\n",
       "         -0.8628,  0.6778, -0.3452, -0.6384, -0.3030,  0.6976, -0.8126,  0.0374,\n",
       "          0.2521,  0.0112,  0.2579,  0.8328, -0.4859, -0.3040,  0.8507, -0.3704,\n",
       "         -0.9284, -0.9175, -0.8694, -0.9624,  0.0552,  0.4014, -0.4648,  0.2608,\n",
       "          0.3528, -0.1300, -0.8374,  0.4060,  0.0693, -0.8745,  0.1197,  0.2500,\n",
       "          0.7264,  0.4752,  0.9059, -0.9024,  0.0828, -0.9780, -0.4988, -0.2412,\n",
       "         -0.1854,  0.8857,  0.5163, -0.7999, -0.8995, -0.9230,  0.8261,  0.5689,\n",
       "          0.6998,  0.3711,  0.4104, -0.6466,  0.7797,  0.8909,  0.7495, -0.0719,\n",
       "         -0.4961, -0.2170, -0.4355, -0.6376, -0.8935,  0.5152,  0.6966,  0.4830,\n",
       "          0.3164,  0.9881, -0.8686, -0.2506, -0.7341, -0.5729,  0.4558, -0.5700,\n",
       "         -0.6931,  0.1183,  0.1815,  0.1509, -0.5849,  0.8124, -0.7133, -0.2103,\n",
       "          0.5228, -0.6813,  0.3354, -0.5075,  0.8855, -0.1500,  0.8754, -0.3089,\n",
       "         -0.1413, -0.8239,  0.2836, -0.9816, -0.2055, -0.8767, -0.5356,  0.6015,\n",
       "         -0.3108,  0.5843,  0.2365, -0.0623, -0.1248,  0.6359,  0.3354,  0.6726,\n",
       "         -0.4388, -0.5753, -0.0858,  0.3541,  0.4660, -0.3797, -0.6888, -0.4238,\n",
       "         -0.0405, -0.5388, -0.0736,  0.0377, -0.5106, -0.9586,  0.5939, -0.2431,\n",
       "          0.1008, -0.5632,  0.0666, -0.6891, -0.3506,  0.2738, -0.1143,  0.0657,\n",
       "          0.4395,  0.7123,  0.9055, -0.4165, -0.5312,  0.6648, -0.7486,  0.1712,\n",
       "          0.2187,  0.8801, -0.8705,  0.0964,  0.0961, -0.8907, -0.1265, -0.7436,\n",
       "          0.1116,  0.1939,  0.6847,  0.0921,  0.7811,  0.1754, -0.4318,  0.6040,\n",
       "          0.6931,  0.9570, -0.0882,  0.5784,  0.7896, -0.1028, -0.3505, -0.4716,\n",
       "         -0.5780,  0.2129, -0.1811,  0.8488, -0.2099, -0.0864, -0.4871,  0.7883,\n",
       "          0.1236, -0.9060, -0.3404,  0.1055,  0.9010, -0.0490, -0.2607,  0.3336,\n",
       "          0.6011, -0.7729, -0.5171,  0.8871,  0.2172,  0.3114, -0.7044,  0.9094,\n",
       "         -0.8279,  0.4149,  0.6367,  0.9362,  0.6337,  0.7184,  0.5505,  0.0596,\n",
       "          0.6342, -0.8196, -0.4388, -0.7432,  0.6020,  0.3784,  0.6193, -0.5006,\n",
       "         -0.6088, -0.1649, -0.7686,  0.3631,  0.6265, -0.4566,  0.1218, -0.7624,\n",
       "          0.4028, -0.5682, -0.0111, -0.3912, -0.7082,  0.2641,  0.3453, -0.8812,\n",
       "         -0.3688,  0.2311,  0.4413,  0.1900,  0.7385,  0.6778, -0.5586, -0.4427,\n",
       "          0.4962, -0.7477,  0.3809,  0.2079, -0.8310, -0.4000, -0.6087, -0.8784,\n",
       "         -0.4829,  0.3609, -0.4447,  0.1677, -0.8770, -0.7176, -0.6828, -0.3305,\n",
       "         -0.3820, -0.4756, -0.7802, -0.2686,  0.5516, -0.3803,  0.5504,  0.0511,\n",
       "          0.3678,  0.0572,  0.5774, -0.7715,  0.7773, -0.1892, -0.7433, -0.4732,\n",
       "         -0.1755, -0.4983, -0.7821, -0.6432,  0.4285, -0.4611, -0.7868,  0.9599,\n",
       "         -0.9495, -0.6504, -0.9689,  0.9606, -0.7710,  0.1904, -0.9321,  0.8895,\n",
       "          0.2635,  0.6864, -0.2651, -0.9687, -0.3705, -0.4620, -0.5867, -0.6559,\n",
       "         -0.8489, -0.1424,  0.6519, -0.9465, -0.2981,  0.8079, -0.8951, -0.9716,\n",
       "          0.2445,  0.9432,  0.0890, -0.6395, -0.5996,  0.7871, -0.6211,  0.2792,\n",
       "          0.2403,  0.8498, -0.9578, -0.6784,  0.2890, -0.9920, -0.2588, -0.0346,\n",
       "          0.4388,  0.3693, -0.8701, -0.4995,  0.8222,  0.8033, -0.5112,  0.5951,\n",
       "         -0.3212, -0.4217,  0.1015,  0.9019,  0.3705,  0.9305,  0.0032, -0.1202,\n",
       "          0.9997,  0.1244, -0.3607,  0.8128,  0.4575,  0.5482,  0.7181,  0.6064,\n",
       "          0.6218, -0.0750, -0.6970, -0.8830, -0.2975, -0.0022, -0.7226,  0.1982,\n",
       "          0.6065,  0.2875,  0.0272,  0.7911, -0.7519, -0.5675, -0.0687,  0.7533,\n",
       "         -0.7510, -0.6659, -0.5990,  0.1544, -0.9015,  0.8856,  0.6317,  0.9049,\n",
       "          0.3101,  0.3258, -0.4795, -0.3743,  0.5438,  0.6045,  0.8281, -0.9382,\n",
       "          0.2802, -0.4333,  0.9453, -0.1306,  0.2008, -0.0633,  0.0769, -0.5999,\n",
       "         -0.3013,  0.1722, -0.8893, -0.8998, -0.3641,  0.8865, -0.9257, -0.7659,\n",
       "          0.2055,  0.3565, -0.9650, -0.2393, -0.2720,  0.8921, -0.0179, -0.2939,\n",
       "          0.2586,  0.2714,  0.3164, -0.3653,  0.3950, -0.0414, -0.4035,  0.4550,\n",
       "          0.4953,  0.2937, -0.2529,  0.6814,  0.1073,  0.0229, -0.8298, -0.0250,\n",
       "          0.8861, -0.7679, -0.3089,  0.3016,  0.3137, -0.1990, -0.4324, -0.8121,\n",
       "         -0.2226, -0.3443,  0.1962,  0.4493, -0.5159, -0.0960, -0.2175, -0.7747,\n",
       "          0.0408, -0.9016,  0.5527,  0.6492, -0.4661, -0.7110, -0.4736, -0.7626,\n",
       "         -0.6883,  0.2066, -0.3419,  0.1544,  0.9363, -0.5184, -0.4048, -0.2630,\n",
       "          0.2792,  0.7271, -0.7435,  0.8874, -0.1839,  0.4347,  0.2256, -0.3202,\n",
       "         -0.4249, -0.4185,  0.8149,  0.3321, -0.4604, -0.1221, -0.2856, -0.8723,\n",
       "          0.0962, -0.5884, -0.2963,  0.0923,  0.4778, -0.6334, -0.3128, -0.6072,\n",
       "         -0.5077,  0.4380,  0.7342, -0.3118,  0.9246, -0.1376, -0.4629,  0.8791,\n",
       "          0.1201, -0.3231,  0.3419, -0.6325, -0.4408, -0.9678, -0.5258, -0.4852,\n",
       "          0.4627, -0.0931, -0.4206, -0.6621,  0.7391,  0.7168, -0.9091,  0.6148,\n",
       "          0.8601,  0.1509,  0.0176,  0.6408, -0.8543, -0.6362,  0.2971,  0.0858,\n",
       "          0.8401,  0.8918, -0.7093, -0.4132,  0.7304, -0.5273,  0.4490,  0.4575,\n",
       "         -0.2280, -0.0647, -0.7240,  0.8647, -0.2501, -0.2335,  0.3498, -0.6129,\n",
       "          0.3349, -0.4346, -0.2822, -0.0918, -0.0203, -0.9108,  0.7848, -0.0682,\n",
       "          0.3279, -0.2017,  0.5664,  0.2020,  0.9584, -0.0611,  0.5209,  0.5479,\n",
       "          0.3560,  0.0880,  0.8031,  0.2301, -0.3160,  0.0189,  0.2245, -0.4057,\n",
       "         -0.6173,  0.9149, -0.2203,  0.5671, -0.3052, -0.8476,  0.9657,  0.9362,\n",
       "         -0.9250, -0.9340,  0.8779, -0.9542,  0.3986, -0.2833, -0.4990,  0.3498,\n",
       "         -0.6720, -0.5277, -0.2865,  0.1543, -0.6482, -0.4568, -0.2570,  0.7731,\n",
       "         -0.5271, -0.7554,  0.8940,  0.2904,  0.3144,  0.2199, -0.6085,  0.9024,\n",
       "          0.1818,  0.4064,  0.6838,  0.4851,  0.0568,  0.7673, -0.7956, -0.7178,\n",
       "         -0.5705, -0.6466,  0.3554,  0.1937, -0.9668, -0.2217, -0.8420,  0.3718,\n",
       "         -0.4248,  0.2259,  0.1919,  0.5019,  0.7171,  0.4637, -0.8837, -0.3806,\n",
       "         -0.0736, -0.3592, -0.5265, -0.7484,  0.0085, -0.5923,  0.3058, -0.0516,\n",
       "         -0.2587,  0.0319, -0.3505,  0.8076,  0.5343,  0.9653, -0.6668,  0.7409,\n",
       "         -0.2619, -0.2993, -0.9807, -0.0261,  0.7688,  0.1820, -0.2745,  0.5905,\n",
       "         -0.0965,  0.2416, -0.3137,  0.4074,  0.2789, -0.3619,  0.7631, -0.3654,\n",
       "         -0.8941, -0.3341,  0.4254, -0.1211,  0.8945, -0.9469,  0.6001, -0.2900,\n",
       "         -0.9857,  0.6927, -0.7208,  0.7528, -0.7653,  0.0791,  0.2932, -0.8392,\n",
       "         -0.1097,  0.8734, -0.8005,  0.2041, -0.1659,  0.5139,  0.1035, -0.4377,\n",
       "         -0.7923, -0.3588, -0.2387, -0.8141,  0.0473, -0.0691, -0.6503,  0.4247,\n",
       "          0.6914, -0.2527,  0.9042, -0.0115, -0.1336, -0.9151,  0.6041, -0.8270,\n",
       "         -0.9717,  0.7730, -0.8124,  0.4223,  0.2482,  0.2361, -0.2575, -0.0858]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = nn.BatchNorm1d(784)\n",
    "bn.training = False\n",
    "bn(torch.tensor(np.expand_dims(s, axis=0), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.running_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brevitas",
   "language": "python",
   "name": "brevitas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
