{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from torch.nn import Module\n",
    "\n",
    "# import brevitas.nn as qnn\n",
    "from brevitas.nn import QuantLinear, QuantHardTanh, QuantIdentity\n",
    "from brevitas.nn import QuantSigmoid\n",
    "from brevitas import config\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantNet(Module):\n",
    "    def __init__(self):\n",
    "        super(QuantNet, self).__init__()\n",
    "\n",
    "        self.fc1 = QuantLinear(784, 128, weight_quant=Int8WeightPerTensorFloatScratch, bias=Int32Bias)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        #self.bn1 = ShiftBatchNorm(1024)\n",
    "\n",
    "        self.fc2 = QuantLinear(128, 128, weight_quant=Int8WeightPerTensorFloatScratch, bias=Int32Bias)\n",
    "        # self.bn2 = nn.BatchNorm1d(1024)\n",
    "        #self.bn2 = ShiftBatchNorm(1024)\n",
    "\n",
    "        self.fc3 = QuantLinear(128, 128, weight_quant=Int8WeightPerTensorFloatScratch, bias=Int32Bias)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        #self.bn3 = ShiftBatchNorm(128)\n",
    "\n",
    "        self.fc4 = QuantLinear(128, 10, weight_quant=Int8WeightPerTensorFloatScratch, bias=Int32Bias)\n",
    "        # self.bn4 = nn.BatchNorm1d(128)\n",
    "        #self.bn4 = ShiftBatchNorm(128)\n",
    "        \n",
    "        self.quant_identity = QuantIdentity(act_quant=Int8ActPerTensorFloatScratch, return_quant_tensor = True)\n",
    "        self.quant_act = QuantSigmoid(act_quant=Int8ActPerTensorFloatScratch, return_quant_tensor = True)\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        x = self.quant_identity(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.quant_act(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.quant_act(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        x = self.quant_act(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        # x = self.bn4(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0078)\n",
      "Parameter containing:\n",
      "tensor(1., requires_grad=True)\n",
      "tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "Parameter containing:\n",
      "tensor(2.8619, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "config.IGNORE_MISSING_KEYS = True\n",
    "# pretrend weight path = 'mnist_qnn_mlp.pt'\n",
    "model_state_dict = torch.load('mnist_qnn_mlp.pt')\n",
    "net = QuantNet()\n",
    "print(net.quant_act.quant_act_scale())\n",
    "print(net.quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)\n",
    "net.load_state_dict(model_state_dict)\n",
    "print(net.quant_act.quant_act_scale())\n",
    "print(net.quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.scaling import ConstScaling\n",
    "\n",
    "from brevitas.inject import ExtendedInjector\n",
    "class CommonQuantizer(ExtendedInjector):\n",
    "    quant_type = QuantType.INT\n",
    "    bit_width_impl_type = BitWidthImplType.CONST\n",
    "    scaling_impl_type = ScalingImplType.CONST\n",
    "    restrict_scaling_type = RestrictValueType.FP\n",
    "    zero_point_impl = ZeroZeroPoint\n",
    "    float_to_int_impl_type = FloatToIntImplType.ROUND\n",
    "    scaling_per_output_channel = False\n",
    "    narrow_range = False\n",
    "    signed = True\n",
    "\n",
    "class ActInferenceQuant(CommonQuantizer, ActQuantSolver):\n",
    "    bit_width = 8\n",
    "    scaling_impl = ConstScaling(2.8619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantNetInference(Module):\n",
    "    def __init__(self):\n",
    "        super(QuantNetInference, self).__init__()\n",
    "\n",
    "        self.fc1 = QuantLinear(784, 128, weight_quant=Int8WeightPerTensorFloatScratch, bias=Int32Bias)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        #self.bn1 = ShiftBatchNorm(1024)\n",
    "\n",
    "        self.fc2 = QuantLinear(128, 128, weight_quant=Int8WeightPerTensorFloatScratch, bias=Int32Bias)\n",
    "        # self.bn2 = nn.BatchNorm1d(1024)\n",
    "        #self.bn2 = ShiftBatchNorm(1024)\n",
    "\n",
    "        self.fc3 = QuantLinear(128, 128, weight_quant=Int8WeightPerTensorFloatScratch, bias=Int32Bias)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        #self.bn3 = ShiftBatchNorm(128)\n",
    "\n",
    "        self.fc4 = QuantLinear(128, 10, weight_quant=Int8WeightPerTensorFloatScratch, bias=Int32Bias)\n",
    "        # self.bn4 = nn.BatchNorm1d(128)\n",
    "        #self.bn4 = ShiftBatchNorm(128)\n",
    "        \n",
    "        self.quant_identity = QuantIdentity(act_quant=ActInferenceQuant, return_quant_tensor = True)\n",
    "        self.quant_act = QuantSigmoid(act_quant=ActInferenceQuant, return_quant_tensor = True)\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        x = self.quant_identity(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.quant_act(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.quant_act(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        x = self.quant_act(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        # x = self.bn4(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantNetInference(\n",
      "  (fc1): QuantLinear(\n",
      "    in_features=784, out_features=128, bias=False\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (output_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (weight_quant): WeightQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): RescalingIntQuant(\n",
      "        (int_quant): IntQuant(\n",
      "          (float_to_int_impl): RoundSte()\n",
      "          (tensor_clamp_impl): TensorClampSte()\n",
      "          (delay_wrapper): DelayWrapper(\n",
      "            (delay_impl): _NoDelay()\n",
      "          )\n",
      "        )\n",
      "        (scaling_impl): StatsFromParameterScaling(\n",
      "          (parameter_list_stats): _ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper(\n",
      "              (view_shape_impl): OverTensorView()\n",
      "            )\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsMax()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): _StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_scaling_pre): Identity()\n",
      "          )\n",
      "        )\n",
      "        (int_scaling_impl): IntScaling()\n",
      "        (zero_point_impl): ZeroZeroPoint(\n",
      "          (zero_point): StatelessBuffer()\n",
      "        )\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "  )\n",
      "  (fc2): QuantLinear(\n",
      "    in_features=128, out_features=128, bias=False\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (output_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (weight_quant): WeightQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): RescalingIntQuant(\n",
      "        (int_quant): IntQuant(\n",
      "          (float_to_int_impl): RoundSte()\n",
      "          (tensor_clamp_impl): TensorClampSte()\n",
      "          (delay_wrapper): DelayWrapper(\n",
      "            (delay_impl): _NoDelay()\n",
      "          )\n",
      "        )\n",
      "        (scaling_impl): StatsFromParameterScaling(\n",
      "          (parameter_list_stats): _ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper(\n",
      "              (view_shape_impl): OverTensorView()\n",
      "            )\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsMax()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): _StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_scaling_pre): Identity()\n",
      "          )\n",
      "        )\n",
      "        (int_scaling_impl): IntScaling()\n",
      "        (zero_point_impl): ZeroZeroPoint(\n",
      "          (zero_point): StatelessBuffer()\n",
      "        )\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "  )\n",
      "  (fc3): QuantLinear(\n",
      "    in_features=128, out_features=128, bias=False\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (output_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (weight_quant): WeightQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): RescalingIntQuant(\n",
      "        (int_quant): IntQuant(\n",
      "          (float_to_int_impl): RoundSte()\n",
      "          (tensor_clamp_impl): TensorClampSte()\n",
      "          (delay_wrapper): DelayWrapper(\n",
      "            (delay_impl): _NoDelay()\n",
      "          )\n",
      "        )\n",
      "        (scaling_impl): StatsFromParameterScaling(\n",
      "          (parameter_list_stats): _ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper(\n",
      "              (view_shape_impl): OverTensorView()\n",
      "            )\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsMax()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): _StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_scaling_pre): Identity()\n",
      "          )\n",
      "        )\n",
      "        (int_scaling_impl): IntScaling()\n",
      "        (zero_point_impl): ZeroZeroPoint(\n",
      "          (zero_point): StatelessBuffer()\n",
      "        )\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "  )\n",
      "  (fc4): QuantLinear(\n",
      "    in_features=128, out_features=10, bias=False\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (output_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (weight_quant): WeightQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): RescalingIntQuant(\n",
      "        (int_quant): IntQuant(\n",
      "          (float_to_int_impl): RoundSte()\n",
      "          (tensor_clamp_impl): TensorClampSte()\n",
      "          (delay_wrapper): DelayWrapper(\n",
      "            (delay_impl): _NoDelay()\n",
      "          )\n",
      "        )\n",
      "        (scaling_impl): StatsFromParameterScaling(\n",
      "          (parameter_list_stats): _ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper(\n",
      "              (view_shape_impl): OverTensorView()\n",
      "            )\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsMax()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): _StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_scaling_pre): Identity()\n",
      "          )\n",
      "        )\n",
      "        (int_scaling_impl): IntScaling()\n",
      "        (zero_point_impl): ZeroZeroPoint(\n",
      "          (zero_point): StatelessBuffer()\n",
      "        )\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "  )\n",
      "  (quant_identity): QuantIdentity(\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (act_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "        (activation_impl): Identity()\n",
      "        (tensor_quant): RescalingIntQuant(\n",
      "          (int_quant): IntQuant(\n",
      "            (float_to_int_impl): RoundSte()\n",
      "            (tensor_clamp_impl): TensorClamp()\n",
      "            (delay_wrapper): DelayWrapper(\n",
      "              (delay_impl): _NoDelay()\n",
      "            )\n",
      "          )\n",
      "          (scaling_impl): ConstScaling(\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): Identity()\n",
      "            )\n",
      "            (value): StatelessBuffer()\n",
      "          )\n",
      "          (int_scaling_impl): IntScaling()\n",
      "          (zero_point_impl): ZeroZeroPoint(\n",
      "            (zero_point): StatelessBuffer()\n",
      "          )\n",
      "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "            (bit_width): StatelessBuffer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (quant_act): QuantSigmoid(\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (act_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "        (activation_impl): Sigmoid()\n",
      "        (tensor_quant): RescalingIntQuant(\n",
      "          (int_quant): IntQuant(\n",
      "            (float_to_int_impl): RoundSte()\n",
      "            (tensor_clamp_impl): TensorClamp()\n",
      "            (delay_wrapper): DelayWrapper(\n",
      "              (delay_impl): _NoDelay()\n",
      "            )\n",
      "          )\n",
      "          (scaling_impl): ConstScaling(\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): Identity()\n",
      "            )\n",
      "            (value): StatelessBuffer()\n",
      "          )\n",
      "          (int_scaling_impl): IntScaling()\n",
      "          (zero_point_impl): ZeroZeroPoint(\n",
      "            (zero_point): StatelessBuffer()\n",
      "          )\n",
      "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "            (bit_width): StatelessBuffer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "quant_model_inference = QuantNetInference()\n",
    "print(quant_model_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before loading\n",
      "tensor(0.0224)\n",
      "tensor(2.8619)\n",
      "after loading\n",
      "tensor(0.0224)\n",
      "tensor(2.8619)\n"
     ]
    }
   ],
   "source": [
    "config.IGNORE_MISSING_KEYS = False\n",
    "# pretrend weight path = 'mnist_qnn_mlp.pt'\n",
    "model_state_dict = torch.load('mnist_qnn_mlp.pt')\n",
    "print(\"before loading\")\n",
    "print(quant_model_inference.quant_act.quant_act_scale())\n",
    "print(quant_model_inference.quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl(torch.empty(1)))\n",
    "\n",
    "quant_model_inference.load_state_dict(model_state_dict)\n",
    "print(\"after loading\")\n",
    "print(quant_model_inference.quant_act.quant_act_scale())\n",
    "print(quant_model_inference.quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl(torch.empty(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random torch tensor\n",
    "x = torch.randn(1, 1, 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model:  QuantTensor(value=tensor([[[[-1.5204, -0.0894,  2.0123, -0.9838, -0.4248,  0.3577,  0.0671,\n",
      "            1.2968,  1.0062,  0.0224,  0.8944,  0.3577,  1.1403,  1.1627,\n",
      "            0.3130,  1.9676,  0.4025, -0.2236, -0.6931,  0.8496,  1.0062,\n",
      "           -0.4695,  0.0000, -0.4248,  0.3354, -1.4757, -1.9005, -0.4472],\n",
      "          [-2.2806,  0.9838,  1.7664, -1.0956,  0.7378,  0.9838,  0.0671,\n",
      "            0.3801, -0.4025, -0.1342, -0.2236,  0.7826, -0.4472,  0.3130,\n",
      "           -0.3130,  2.1017, -1.4310, -1.1179,  2.0347,  0.3354,  0.1565,\n",
      "            0.5366,  0.0671,  0.3801,  2.2359, -0.4695, -1.6322,  0.8720],\n",
      "          [ 1.2968, -0.2459, -0.9838,  1.4757, -0.3801, -0.9838,  0.1789,\n",
      "           -0.5590, -0.8944, -0.2683,  0.5143, -0.3577,  0.6484, -0.2683,\n",
      "            1.3415,  1.2074, -0.2907, -0.1342, -1.2074,  0.8944, -0.4248,\n",
      "            1.6769, -0.7826, -1.7440,  0.1342, -0.9167,  1.2297,  0.6037],\n",
      "          [ 0.9167,  0.4472,  0.9391,  1.3863,  0.4472, -1.4310, -0.4472,\n",
      "            1.2521,  0.5813,  0.8496,  0.8049, -0.9614, -1.5875, -0.2236,\n",
      "            0.4919, -0.3577, -1.4086, -0.6261, -0.3130, -0.5143,  0.8496,\n",
      "           -0.9838, -1.5204, -0.8944, -2.0570,  1.3415,  1.5651, -0.8049],\n",
      "          [-0.5590, -0.0894, -0.2459,  1.0956, -0.2683,  1.7664,  0.1342,\n",
      "           -0.1789,  0.2683, -0.7155,  0.2907,  0.6484, -0.1565, -0.2683,\n",
      "           -0.4472,  1.1403,  0.0447,  1.2968, -0.2236,  1.3863, -0.0000,\n",
      "            0.1342,  0.0671,  0.6261,  0.1565, -0.7378,  0.0224,  1.7887],\n",
      "          [ 0.6931,  1.4981,  0.1118,  0.4248, -0.2236, -0.5366,  0.9391,\n",
      "            0.8049, -0.2459, -0.4472,  1.7216, -0.8496, -0.0671,  0.0000,\n",
      "           -1.2745, -0.6708,  1.1850, -0.1342,  0.7602,  1.8111, -0.2459,\n",
      "           -0.6708, -0.4025,  0.3130,  0.9614, -0.7155,  1.6322, -0.4248],\n",
      "          [ 0.5366,  0.1789, -0.6931, -0.2012,  1.8111,  1.0956,  0.3130,\n",
      "            0.3801,  0.3801,  1.2745,  0.2907,  0.3801,  0.7155, -0.6037,\n",
      "            0.3801,  0.5366,  0.4248,  0.2683, -0.0894,  0.0224,  1.0956,\n",
      "            0.3130, -0.7602,  0.0000,  0.5143,  0.8944, -0.7602,  0.5143],\n",
      "          [ 0.4472,  0.2907, -0.6484,  0.6037,  0.4472, -0.5813, -0.5143,\n",
      "            2.0347, -0.6261, -0.3130,  1.1850,  1.0285,  1.0732,  0.8496,\n",
      "            1.0732,  0.8273, -0.1565, -1.9229,  1.4310, -0.7155,  0.7155,\n",
      "            0.0671,  0.9391, -0.6484, -2.2806,  0.0894, -0.2012, -0.8720],\n",
      "          [-0.9391,  0.2459,  0.5143, -1.6546, -0.1565, -0.4919,  0.1789,\n",
      "            0.2907, -1.6098,  0.8273,  0.8944, -2.3701,  1.8111,  0.0671,\n",
      "            1.2745, -0.7378, -1.0732,  0.0671, -2.8172, -1.0062, -1.0509,\n",
      "           -1.0285, -0.5813,  0.6261,  1.5651, -0.5366,  1.2745, -2.6384],\n",
      "          [ 0.3354,  1.2074, -0.0224,  0.9167, -1.2297, -0.2236, -1.4981,\n",
      "           -0.2236, -0.2907,  1.2074, -1.3639, -0.2236,  0.1789,  0.5590,\n",
      "           -0.7155,  0.9167, -0.9838, -2.2359, -0.4025, -0.4025, -1.0509,\n",
      "           -0.5813, -2.2359, -0.1342,  0.3130,  0.2683,  0.7155,  0.7378],\n",
      "          [ 0.0447, -0.3577, -0.4248,  0.7378, -1.5651, -0.4248, -0.0671,\n",
      "           -0.3801,  0.2683, -0.8273,  0.1565, -0.0894,  0.2907,  1.4310,\n",
      "           -0.0894,  0.7602, -0.9167,  0.6261, -0.5366,  0.6484, -0.4695,\n",
      "            0.9614,  0.7378, -0.3801,  2.1241, -1.0509, -0.3577,  1.1850],\n",
      "          [-1.2968,  0.0224,  2.8396, -0.1789,  0.2459, -1.0732, -0.4919,\n",
      "            0.4695, -2.2583, -1.1403, -0.9391, -0.8944, -0.7155,  0.9614,\n",
      "           -0.0447, -0.1118,  2.4371, -1.1850, -0.4248, -1.3192, -0.8273,\n",
      "           -0.5590,  0.3577,  0.4248, -0.1342,  1.0285, -1.2297,  0.3354],\n",
      "          [-1.4981,  0.5590, -0.6708, -0.1118,  1.2521, -0.8496, -0.7378,\n",
      "            0.4025, -0.7155,  1.3639,  1.3639,  0.5813, -0.0224, -1.1403,\n",
      "           -0.8496, -1.4981,  0.4919, -0.7378,  0.1565,  0.0447, -0.4695,\n",
      "            1.4757,  0.5590,  1.0062,  0.8273,  0.1789,  0.5143, -0.4025],\n",
      "          [ 0.7378,  0.3801, -0.8496, -0.4695, -0.9391,  2.3701,  1.2745,\n",
      "            1.6098,  0.9614,  0.6708, -0.4248,  0.4695, -1.3639,  0.3801,\n",
      "           -1.4310, -0.1118,  0.4248,  0.7826,  1.7664,  0.2683,  0.0224,\n",
      "           -0.4248, -0.6931,  0.1342,  0.3577, -1.5651,  1.2074,  0.7378],\n",
      "          [ 0.1342,  0.1789,  0.0671, -1.1403, -0.2683,  0.8720,  0.3577,\n",
      "           -0.1789, -1.1179,  0.4248,  1.6546,  0.4919,  0.0224,  0.4695,\n",
      "           -1.0732,  1.2074, -0.1565, -0.0671, -0.0447,  0.2907, -0.4472,\n",
      "            0.4919,  2.1912, -1.9899,  0.8049, -0.2907,  0.3354,  0.9391],\n",
      "          [ 1.6546,  0.1118, -2.0123, -0.0894, -1.6098, -0.1118,  0.0671,\n",
      "            1.4086,  0.3130, -0.1565,  0.1118, -1.0062,  0.4472, -0.2907,\n",
      "           -0.7378,  0.6037, -0.9167, -0.2012,  0.4248,  0.1789,  1.3639,\n",
      "            0.6037,  1.3415,  0.7826, -0.1118,  0.1565,  0.3354,  1.1627],\n",
      "          [ 0.3801,  2.2135,  1.6546, -0.2012, -0.0894,  0.0671, -0.8049,\n",
      "           -1.8558, -1.3415, -0.0671, -0.5143, -0.8496,  0.0894,  0.8049,\n",
      "            0.1565,  1.2968, -0.1342,  1.0285,  1.4981,  0.4695,  1.0285,\n",
      "            1.1850, -0.8273, -0.0000,  0.3130, -0.1789, -0.5366,  0.8273],\n",
      "          [ 0.6931, -0.3130,  1.0956, -0.6931,  0.6708, -0.7378,  2.0347,\n",
      "           -1.3192, -0.3801,  0.1342, -1.0509, -0.0447,  0.3801,  0.9614,\n",
      "           -0.8944,  0.4248,  0.4695, -1.8111, -0.6037,  0.3801, -1.3415,\n",
      "            2.5713, -0.6708, -0.7826,  1.2968, -0.4025,  0.3354,  0.2907],\n",
      "          [-0.6484,  0.4025, -0.1118,  0.2459, -0.6708, -0.4472, -0.4919,\n",
      "            0.6037, -1.3863, -1.9005, -0.6931,  2.3253, -0.0894, -1.0732,\n",
      "            1.1850,  0.6708, -0.0894, -0.6484,  1.0509,  0.7155,  1.1627,\n",
      "            0.7155,  1.5428,  0.4695, -0.9614, -0.4695,  0.0447, -0.6931],\n",
      "          [ 1.0956, -2.1017, -0.1342,  1.8558,  1.2297, -1.0956,  2.7054,\n",
      "           -0.2907, -0.4695, -0.5813,  0.4695,  1.6993,  0.0894,  1.1179,\n",
      "            0.2683, -1.0956,  2.6160, -0.2236, -0.0894, -1.5204,  0.5590,\n",
      "           -1.9452, -0.3801, -0.1565, -1.0509,  2.6160, -0.1565, -0.2236],\n",
      "          [-0.8049, -0.5366, -0.1118, -0.8944,  0.5143, -0.4025,  0.7602,\n",
      "           -0.8049,  2.3030, -0.0671,  0.3354,  0.6708, -1.5428,  0.2236,\n",
      "            0.2236, -0.6261, -0.2236, -0.0224, -0.4472, -0.8944,  0.6708,\n",
      "            0.2236, -0.6931,  0.1565,  0.0894,  2.0794,  0.5590,  1.4310],\n",
      "          [ 0.6708, -0.2012,  0.4025,  1.0285, -0.7826, -0.1342, -1.4981,\n",
      "           -0.0447,  1.8782, -0.5366,  0.2012,  2.8396,  0.2683,  0.2236,\n",
      "           -0.2459, -1.8111, -2.3253, -0.1789,  0.0671,  0.7378, -2.1241,\n",
      "           -1.9899,  0.4025,  0.6931, -0.2459, -0.8273, -0.1342,  1.2968],\n",
      "          [-0.2459,  0.3801,  0.3577, -0.6931,  0.5366, -1.7887, -1.2968,\n",
      "           -1.5651,  0.6931, -0.2683, -0.4248, -0.0894,  1.1179, -0.2683,\n",
      "            2.4818, -0.0224,  1.9899,  1.0062, -0.4025,  2.7502, -1.1850,\n",
      "            1.8111,  0.9391,  1.8111, -0.9391,  2.4371,  1.1403,  0.6484],\n",
      "          [-0.9614,  0.2459, -0.0671,  0.2907,  0.0447,  1.8782,  0.0447,\n",
      "           -0.3130,  0.5813,  0.9838,  0.6484, -0.0671, -0.0447,  0.4025,\n",
      "           -0.5366,  0.0894, -1.0062, -0.4695, -1.3192,  0.7155, -0.3577,\n",
      "            0.2236,  1.5428, -0.8720, -0.4472,  0.4695, -0.8944, -0.3354],\n",
      "          [-0.9614,  0.7155,  0.2683,  0.3801, -1.7887, -0.4248, -0.6484,\n",
      "            0.5143,  0.7602,  0.7378,  0.0000,  0.4919, -0.1565, -1.3639,\n",
      "           -0.6931,  1.1179, -0.3354,  0.0894, -0.2012,  1.0956,  2.0123,\n",
      "           -0.2012, -0.8720, -1.6546, -0.3354,  1.3863,  0.7378,  1.3192],\n",
      "          [-0.9391,  0.4919, -0.0224,  0.2907, -0.6037, -0.1118,  0.4025,\n",
      "           -0.7602, -1.6769, -1.4981,  2.0123, -1.3192,  1.0732, -1.0732,\n",
      "            1.3415,  0.3801,  0.6708,  0.7378,  0.0671, -0.2012, -1.3863,\n",
      "            0.3354, -0.0224,  0.9614,  1.3639,  0.5813,  1.5204, -1.2074],\n",
      "          [-0.0000,  0.6261,  0.9838,  1.0062, -0.5143,  0.6931,  0.5813,\n",
      "           -0.4248,  0.8496,  0.9614,  0.6484, -0.9614,  1.2074,  0.7378,\n",
      "           -0.0447, -0.6261,  0.4695,  0.4248, -0.4919,  0.2012,  0.6484,\n",
      "            0.2012, -0.2459,  0.2907,  0.3577,  0.2236, -1.0509, -0.4695],\n",
      "          [-1.4981,  0.4025, -0.1118, -1.0285,  0.3577, -0.4025,  0.5366,\n",
      "           -1.1627, -1.5428,  1.0285,  0.4025, -1.1627, -0.3130, -1.2521,\n",
      "           -0.6484, -1.3415, -0.2236, -0.3577,  1.1850,  0.3354,  0.2907,\n",
      "            0.2907,  0.0224, -0.7602, -1.3639, -0.5143, -0.3801,  0.0671]]]],\n",
      "       grad_fn=<MulBackward0>), scale=tensor(0.0224, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmlin/miniconda3/envs/torch112/lib/python3.8/site-packages/torch/_tensor.py:1362: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392036766/work/c10/core/TensorImpl.h:1900.)\n",
      "  return super().rename(names)\n"
     ]
    }
   ],
   "source": [
    "print(\"training model: \", net.quant_identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference model:  QuantTensor(value=tensor([[[[-1.5204, -0.0894,  2.0123, -0.9838, -0.4248,  0.3577,  0.0671,\n",
      "            1.2968,  1.0061,  0.0224,  0.8943,  0.3577,  1.1403,  1.1626,\n",
      "            0.3130,  1.9676,  0.4025, -0.2236, -0.6931,  0.8496,  1.0061,\n",
      "           -0.4695,  0.0000, -0.4248,  0.3354, -1.4757, -1.9005, -0.4472],\n",
      "          [-2.2806,  0.9838,  1.7663, -1.0956,  0.7378,  0.9838,  0.0671,\n",
      "            0.3801, -0.4025, -0.1342, -0.2236,  0.7826, -0.4472,  0.3130,\n",
      "           -0.3130,  2.1017, -1.4310, -1.1179,  2.0346,  0.3354,  0.1565,\n",
      "            0.5366,  0.0671,  0.3801,  2.2359, -0.4695, -1.6322,  0.8720],\n",
      "          [ 1.2968, -0.2459, -0.9838,  1.4757, -0.3801, -0.9838,  0.1789,\n",
      "           -0.5590, -0.8943, -0.2683,  0.5142, -0.3577,  0.6484, -0.2683,\n",
      "            1.3415,  1.2074, -0.2907, -0.1342, -1.2074,  0.8943, -0.4248,\n",
      "            1.6769, -0.7826, -1.7440,  0.1342, -0.9167,  1.2297,  0.6037],\n",
      "          [ 0.9167,  0.4472,  0.9391,  1.3862,  0.4472, -1.4310, -0.4472,\n",
      "            1.2521,  0.5813,  0.8496,  0.8049, -0.9614, -1.5875, -0.2236,\n",
      "            0.4919, -0.3577, -1.4086, -0.6260, -0.3130, -0.5142,  0.8496,\n",
      "           -0.9838, -1.5204, -0.8943, -2.0570,  1.3415,  1.5651, -0.8049],\n",
      "          [-0.5590, -0.0894, -0.2459,  1.0956, -0.2683,  1.7663,  0.1342,\n",
      "           -0.1789,  0.2683, -0.7155,  0.2907,  0.6484, -0.1565, -0.2683,\n",
      "           -0.4472,  1.1403,  0.0447,  1.2968, -0.2236,  1.3862, -0.0000,\n",
      "            0.1342,  0.0671,  0.6260,  0.1565, -0.7378,  0.0224,  1.7887],\n",
      "          [ 0.6931,  1.4980,  0.1118,  0.4248, -0.2236, -0.5366,  0.9391,\n",
      "            0.8049, -0.2459, -0.4472,  1.7216, -0.8496, -0.0671,  0.0000,\n",
      "           -1.2744, -0.6708,  1.1850, -0.1342,  0.7602,  1.8110, -0.2459,\n",
      "           -0.6708, -0.4025,  0.3130,  0.9614, -0.7155,  1.6322, -0.4248],\n",
      "          [ 0.5366,  0.1789, -0.6931, -0.2012,  1.8110,  1.0956,  0.3130,\n",
      "            0.3801,  0.3801,  1.2744,  0.2907,  0.3801,  0.7155, -0.6037,\n",
      "            0.3801,  0.5366,  0.4248,  0.2683, -0.0894,  0.0224,  1.0956,\n",
      "            0.3130, -0.7602,  0.0000,  0.5142,  0.8943, -0.7602,  0.5142],\n",
      "          [ 0.4472,  0.2907, -0.6484,  0.6037,  0.4472, -0.5813, -0.5142,\n",
      "            2.0346, -0.6260, -0.3130,  1.1850,  1.0285,  1.0732,  0.8496,\n",
      "            1.0732,  0.8273, -0.1565, -1.9228,  1.4310, -0.7155,  0.7155,\n",
      "            0.0671,  0.9391, -0.6484, -2.2806,  0.0894, -0.2012, -0.8720],\n",
      "          [-0.9391,  0.2459,  0.5142, -1.6545, -0.1565, -0.4919,  0.1789,\n",
      "            0.2907, -1.6098,  0.8273,  0.8943, -2.3700,  1.8110,  0.0671,\n",
      "            1.2744, -0.7378, -1.0732,  0.0671, -2.8172, -1.0061, -1.0509,\n",
      "           -1.0285, -0.5813,  0.6260,  1.5651, -0.5366,  1.2744, -2.6383],\n",
      "          [ 0.3354,  1.2074, -0.0224,  0.9167, -1.2297, -0.2236, -1.4980,\n",
      "           -0.2236, -0.2907,  1.2074, -1.3639, -0.2236,  0.1789,  0.5590,\n",
      "           -0.7155,  0.9167, -0.9838, -2.2359, -0.4025, -0.4025, -1.0509,\n",
      "           -0.5813, -2.2359, -0.1342,  0.3130,  0.2683,  0.7155,  0.7378],\n",
      "          [ 0.0447, -0.3577, -0.4248,  0.7378, -1.5651, -0.4248, -0.0671,\n",
      "           -0.3801,  0.2683, -0.8273,  0.1565, -0.0894,  0.2907,  1.4310,\n",
      "           -0.0894,  0.7602, -0.9167,  0.6260, -0.5366,  0.6484, -0.4695,\n",
      "            0.9614,  0.7378, -0.3801,  2.1241, -1.0509, -0.3577,  1.1850],\n",
      "          [-1.2968,  0.0224,  2.8395, -0.1789,  0.2459, -1.0732, -0.4919,\n",
      "            0.4695, -2.2582, -1.1403, -0.9391, -0.8943, -0.7155,  0.9614,\n",
      "           -0.0447, -0.1118,  2.4371, -1.1850, -0.4248, -1.3192, -0.8273,\n",
      "           -0.5590,  0.3577,  0.4248, -0.1342,  1.0285, -1.2297,  0.3354],\n",
      "          [-1.4980,  0.5590, -0.6708, -0.1118,  1.2521, -0.8496, -0.7378,\n",
      "            0.4025, -0.7155,  1.3639,  1.3639,  0.5813, -0.0224, -1.1403,\n",
      "           -0.8496, -1.4980,  0.4919, -0.7378,  0.1565,  0.0447, -0.4695,\n",
      "            1.4757,  0.5590,  1.0061,  0.8273,  0.1789,  0.5142, -0.4025],\n",
      "          [ 0.7378,  0.3801, -0.8496, -0.4695, -0.9391,  2.3700,  1.2744,\n",
      "            1.6098,  0.9614,  0.6708, -0.4248,  0.4695, -1.3639,  0.3801,\n",
      "           -1.4310, -0.1118,  0.4248,  0.7826,  1.7663,  0.2683,  0.0224,\n",
      "           -0.4248, -0.6931,  0.1342,  0.3577, -1.5651,  1.2074,  0.7378],\n",
      "          [ 0.1342,  0.1789,  0.0671, -1.1403, -0.2683,  0.8720,  0.3577,\n",
      "           -0.1789, -1.1179,  0.4248,  1.6545,  0.4919,  0.0224,  0.4695,\n",
      "           -1.0732,  1.2074, -0.1565, -0.0671, -0.0447,  0.2907, -0.4472,\n",
      "            0.4919,  2.1911, -1.9899,  0.8049, -0.2907,  0.3354,  0.9391],\n",
      "          [ 1.6545,  0.1118, -2.0123, -0.0894, -1.6098, -0.1118,  0.0671,\n",
      "            1.4086,  0.3130, -0.1565,  0.1118, -1.0061,  0.4472, -0.2907,\n",
      "           -0.7378,  0.6037, -0.9167, -0.2012,  0.4248,  0.1789,  1.3639,\n",
      "            0.6037,  1.3415,  0.7826, -0.1118,  0.1565,  0.3354,  1.1626],\n",
      "          [ 0.3801,  2.2135,  1.6545, -0.2012, -0.0894,  0.0671, -0.8049,\n",
      "           -1.8558, -1.3415, -0.0671, -0.5142, -0.8496,  0.0894,  0.8049,\n",
      "            0.1565,  1.2968, -0.1342,  1.0285,  1.4980,  0.4695,  1.0285,\n",
      "            1.1850, -0.8273, -0.0000,  0.3130, -0.1789, -0.5366,  0.8273],\n",
      "          [ 0.6931, -0.3130,  1.0956, -0.6931,  0.6708, -0.7378,  2.0346,\n",
      "           -1.3192, -0.3801,  0.1342, -1.0509, -0.0447,  0.3801,  0.9614,\n",
      "           -0.8943,  0.4248,  0.4695, -1.8110, -0.6037,  0.3801, -1.3415,\n",
      "            2.5712, -0.6708, -0.7826,  1.2968, -0.4025,  0.3354,  0.2907],\n",
      "          [-0.6484,  0.4025, -0.1118,  0.2459, -0.6708, -0.4472, -0.4919,\n",
      "            0.6037, -1.3862, -1.9005, -0.6931,  2.3253, -0.0894, -1.0732,\n",
      "            1.1850,  0.6708, -0.0894, -0.6484,  1.0509,  0.7155,  1.1626,\n",
      "            0.7155,  1.5427,  0.4695, -0.9614, -0.4695,  0.0447, -0.6931],\n",
      "          [ 1.0956, -2.1017, -0.1342,  1.8558,  1.2297, -1.0956,  2.7054,\n",
      "           -0.2907, -0.4695, -0.5813,  0.4695,  1.6993,  0.0894,  1.1179,\n",
      "            0.2683, -1.0956,  2.6160, -0.2236, -0.0894, -1.5204,  0.5590,\n",
      "           -1.9452, -0.3801, -0.1565, -1.0509,  2.6160, -0.1565, -0.2236],\n",
      "          [-0.8049, -0.5366, -0.1118, -0.8943,  0.5142, -0.4025,  0.7602,\n",
      "           -0.8049,  2.3029, -0.0671,  0.3354,  0.6708, -1.5427,  0.2236,\n",
      "            0.2236, -0.6260, -0.2236, -0.0224, -0.4472, -0.8943,  0.6708,\n",
      "            0.2236, -0.6931,  0.1565,  0.0894,  2.0793,  0.5590,  1.4310],\n",
      "          [ 0.6708, -0.2012,  0.4025,  1.0285, -0.7826, -0.1342, -1.4980,\n",
      "           -0.0447,  1.8781, -0.5366,  0.2012,  2.8395,  0.2683,  0.2236,\n",
      "           -0.2459, -1.8110, -2.3253, -0.1789,  0.0671,  0.7378, -2.1241,\n",
      "           -1.9899,  0.4025,  0.6931, -0.2459, -0.8273, -0.1342,  1.2968],\n",
      "          [-0.2459,  0.3801,  0.3577, -0.6931,  0.5366, -1.7887, -1.2968,\n",
      "           -1.5651,  0.6931, -0.2683, -0.4248, -0.0894,  1.1179, -0.2683,\n",
      "            2.4818, -0.0224,  1.9899,  1.0061, -0.4025,  2.7501, -1.1850,\n",
      "            1.8110,  0.9391,  1.8110, -0.9391,  2.4371,  1.1403,  0.6484],\n",
      "          [-0.9614,  0.2459, -0.0671,  0.2907,  0.0447,  1.8781,  0.0447,\n",
      "           -0.3130,  0.5813,  0.9838,  0.6484, -0.0671, -0.0447,  0.4025,\n",
      "           -0.5366,  0.0894, -1.0061, -0.4695, -1.3192,  0.7155, -0.3577,\n",
      "            0.2236,  1.5427, -0.8720, -0.4472,  0.4695, -0.8943, -0.3354],\n",
      "          [-0.9614,  0.7155,  0.2683,  0.3801, -1.7887, -0.4248, -0.6484,\n",
      "            0.5142,  0.7602,  0.7378,  0.0000,  0.4919, -0.1565, -1.3639,\n",
      "           -0.6931,  1.1179, -0.3354,  0.0894, -0.2012,  1.0956,  2.0123,\n",
      "           -0.2012, -0.8720, -1.6545, -0.3354,  1.3862,  0.7378,  1.3192],\n",
      "          [-0.9391,  0.4919, -0.0224,  0.2907, -0.6037, -0.1118,  0.4025,\n",
      "           -0.7602, -1.6769, -1.4980,  2.0123, -1.3192,  1.0732, -1.0732,\n",
      "            1.3415,  0.3801,  0.6708,  0.7378,  0.0671, -0.2012, -1.3862,\n",
      "            0.3354, -0.0224,  0.9614,  1.3639,  0.5813,  1.5204, -1.2074],\n",
      "          [-0.0000,  0.6260,  0.9838,  1.0061, -0.5142,  0.6931,  0.5813,\n",
      "           -0.4248,  0.8496,  0.9614,  0.6484, -0.9614,  1.2074,  0.7378,\n",
      "           -0.0447, -0.6260,  0.4695,  0.4248, -0.4919,  0.2012,  0.6484,\n",
      "            0.2012, -0.2459,  0.2907,  0.3577,  0.2236, -1.0509, -0.4695],\n",
      "          [-1.4980,  0.4025, -0.1118, -1.0285,  0.3577, -0.4025,  0.5366,\n",
      "           -1.1626, -1.5427,  1.0285,  0.4025, -1.1626, -0.3130, -1.2521,\n",
      "           -0.6484, -1.3415, -0.2236, -0.3577,  1.1850,  0.3354,  0.2907,\n",
      "            0.2907,  0.0224, -0.7602, -1.3639, -0.5142, -0.3801,  0.0671]]]]), scale=tensor(0.0224), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))\n"
     ]
    }
   ],
   "source": [
    "print(\"inference model: \", quant_model_inference.quant_identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(net.quant_identity(x).value - quant_model_inference.quant_identity(x).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
